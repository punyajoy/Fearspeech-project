{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networkit import *\n",
    "import glob\n",
    "from tqdm import tqdm_notebook\n",
    "import networkit as nx\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import datetime\n",
    "import pickle\n",
    "from glob import glob\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.setNumberOfThreads(20)\n",
    "nx.setLogLevel(\"TRACE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../user_details.json') as fp:\n",
    "    dict_users_original = json.load(fp)\n",
    "with open('../Networks/interaction_network_per_month.json', 'rb') as fp:\n",
    "    data = pickle.load(fp)\n",
    "with open('../Networks/kcore_temportal.json', 'rb') as fp:\n",
    "    monthwise_kcore_dict = pickle.load(fp)  \n",
    "with open('../Networks/map_dict_kcore.json', 'rb') as fp:\n",
    "    map_dict_kcore = pickle.load(fp)  \n",
    "    \n",
    "df_user_label=pd.read_csv('../Results/user_labelling.csv')\n",
    "all_normal_dict={key:1 for key in list(df_user_label[df_user_label['label']=='normal']['user_key'])}\n",
    "# df_matched=pd.read_csv('../Results/matched.csv')\n",
    "df_matched=pd.read_csv('../Results/matched_fear_hate.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Networks/interaction_network_per_month.json', 'rb') as fp:\n",
    "    interaction_data = pickle.load(fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Networks/betweeness_centrality.json', 'rb') as fp:\n",
    "    betweenness_data = pickle.load(fp)\n",
    "    \n",
    "with open('../Networks/eigen_vector_centrality.json', 'rb') as fp:\n",
    "    eigen_data = pickle.load(fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_betweenness=pd.DataFrame(betweenness_data.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users_timeline=pd.read_csv('../Results/kcore_analysis.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = '../../../../GAB_data/FollowersFollowing/Follow_Following_June2018.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold={'hatespeech':0.9,'fearspeech':0.7,'normal':0.7}\n",
    "dict_labels={'hatespeech':2,'fearspeech':1,'normal':0}\n",
    "reverse_dict_labels={dict_labels[key]:key for key in dict_labels.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-087242fa3de8>:6: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for file in tqdm_notebook(files,total=len(files)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7091a9b6159c4f2580019d2204875b31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/212 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dict_users={}\n",
    "dict_users_ambi={}\n",
    "files = sorted(glob('../../Gab_Data/new_features_old_gab/gab_fear_hate_features*.pickle'))\n",
    "\n",
    "\n",
    "dict_dates={}\n",
    "for file in tqdm_notebook(files,total=len(files)):\n",
    "    \n",
    "    with open(file, 'rb') as handle:\n",
    "        Gab_keyword_match = pickle.load(handle)\n",
    "    \n",
    "    if('predicted_probab' not in Gab_keyword_match[1].keys()):\n",
    "        continue\n",
    "    for element in Gab_keyword_match:\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            dt = datetime.fromisoformat(element['post_create_time'])\n",
    "            key_new=str(dt.month)+'/'+str(dt.year)\n",
    "        except KeyError:\n",
    "            continue\n",
    "\n",
    "    \n",
    "        \n",
    "        labels=[]\n",
    "        for i in range(len(element['predicted_probab'])):\n",
    "            if(element['predicted_probab'][i]>threshold[reverse_dict_labels[i]]):\n",
    "                labels.append(reverse_dict_labels[i])\n",
    "        \n",
    "        labels_ambi=[]\n",
    "        for i in range(len(element['predicted_probab'])):\n",
    "            if(element['predicted_probab'][i]>0.5):\n",
    "                labels_ambi.append(reverse_dict_labels[i])\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            for label in labels:\n",
    "                dict_users[element['username']][label]+=1\n",
    "            dict_users[element['username']]['total']+=1\n",
    "        except KeyError:\n",
    "            try:\n",
    "                dict_users[element['username']]={'fearspeech':0,'hatespeech':0,'normal':0,'total':0}\n",
    "                for label in labels:\n",
    "                    dict_users[element['username']][label]+=1\n",
    "                dict_users[element['username']]['total']+=1\n",
    "\n",
    "            except KeyError:\n",
    "                pass\n",
    "#         try:\n",
    "#             for label in labels:\n",
    "#                 dict_users_ambi[element['username']][label]+=1\n",
    "#             dict_users_ambi[element['username']]['total']+=1\n",
    "#         except KeyError:\n",
    "#             dict_users_ambi[element['username']]={'fearspeech':0,'hatespeech':0,'normal':0,'total':0}\n",
    "#             for label in labels:\n",
    "#                 dict_users_ambi[element['username']][label]+=1\n",
    "#             dict_users_ambi[element['username']]['total']+=1\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_users_filter={}\n",
    "max_fear=0\n",
    "max_hate=0\n",
    "for key in dict_users.keys():\n",
    "    user=dict_users[key]\n",
    "    total=user['fearspeech']+user['hatespeech']\n",
    "    \n",
    "    if(user['fearspeech']>max_fear):\n",
    "        max_fear=user['fearspeech']\n",
    "    \n",
    "    if(user['hatespeech']>max_hate):\n",
    "        max_hate=user['hatespeech']\n",
    "    \n",
    "    if(total>=10):\n",
    "        dict_users_filter[key]=user\n",
    "        dict_users_filter[key]['name']=key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_users=pd.DataFrame(dict_users_filter).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fearspeech</th>\n",
       "      <th>hatespeech</th>\n",
       "      <th>normal</th>\n",
       "      <th>total</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>libtardOZ</th>\n",
       "      <td>105</td>\n",
       "      <td>33</td>\n",
       "      <td>428</td>\n",
       "      <td>871</td>\n",
       "      <td>libtardOZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>libtardOZ</th>\n",
       "      <td>105</td>\n",
       "      <td>33</td>\n",
       "      <td>428</td>\n",
       "      <td>871</td>\n",
       "      <td>libtardOZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EyesBlue71</th>\n",
       "      <td>196</td>\n",
       "      <td>6</td>\n",
       "      <td>690</td>\n",
       "      <td>1340</td>\n",
       "      <td>EyesBlue71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1488Goy</th>\n",
       "      <td>303</td>\n",
       "      <td>130</td>\n",
       "      <td>651</td>\n",
       "      <td>2090</td>\n",
       "      <td>1488Goy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ArtificeCubed</th>\n",
       "      <td>93</td>\n",
       "      <td>14</td>\n",
       "      <td>2697</td>\n",
       "      <td>3659</td>\n",
       "      <td>ArtificeCubed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ramenanne</th>\n",
       "      <td>116</td>\n",
       "      <td>43</td>\n",
       "      <td>265</td>\n",
       "      <td>853</td>\n",
       "      <td>ramenanne</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Obbop</th>\n",
       "      <td>146</td>\n",
       "      <td>20</td>\n",
       "      <td>87</td>\n",
       "      <td>389</td>\n",
       "      <td>Obbop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Obbop</th>\n",
       "      <td>146</td>\n",
       "      <td>20</td>\n",
       "      <td>87</td>\n",
       "      <td>389</td>\n",
       "      <td>Obbop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Obbop</th>\n",
       "      <td>146</td>\n",
       "      <td>20</td>\n",
       "      <td>87</td>\n",
       "      <td>389</td>\n",
       "      <td>Obbop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Obbop</th>\n",
       "      <td>146</td>\n",
       "      <td>20</td>\n",
       "      <td>87</td>\n",
       "      <td>389</td>\n",
       "      <td>Obbop</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>476 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              fearspeech hatespeech normal total           name\n",
       "libtardOZ            105         33    428   871      libtardOZ\n",
       "libtardOZ            105         33    428   871      libtardOZ\n",
       "EyesBlue71           196          6    690  1340     EyesBlue71\n",
       "1488Goy              303        130    651  2090        1488Goy\n",
       "ArtificeCubed         93         14   2697  3659  ArtificeCubed\n",
       "...                  ...        ...    ...   ...            ...\n",
       "ramenanne            116         43    265   853      ramenanne\n",
       "Obbop                146         20     87   389          Obbop\n",
       "Obbop                146         20     87   389          Obbop\n",
       "Obbop                146         20     87   389          Obbop\n",
       "Obbop                146         20     87   389          Obbop\n",
       "\n",
       "[476 rows x 5 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final_users.loc[list(df_matched.control)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_users.to_csv('../Results/user_distribution.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_bd=[]\n",
    "list_ev=[]\n",
    "list_followers=[]\n",
    "list_following=[]\n",
    "\n",
    "\n",
    "for index,row in df_final_users.iterrows():\n",
    "    list_bd.append(betweenness_data[index])\n",
    "    list_ev.append(eigen_data[index])\n",
    "    list_following.append(dict_users_original[index]['followers_count'])\n",
    "    list_followers.append(dict_users_original[index]['followings_count'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_users['eigen_vector']=list_ev\n",
    "df_final_users['betweeness']=list_bd\n",
    "df_final_users['followers']=list_followers\n",
    "df_final_users['following']=list_following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fearspeech</th>\n",
       "      <th>hatespeech</th>\n",
       "      <th>normal</th>\n",
       "      <th>total</th>\n",
       "      <th>name</th>\n",
       "      <th>eigen_vector</th>\n",
       "      <th>betweeness</th>\n",
       "      <th>followers</th>\n",
       "      <th>following</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>radiofan2</th>\n",
       "      <td>47</td>\n",
       "      <td>26</td>\n",
       "      <td>1716</td>\n",
       "      <td>2287</td>\n",
       "      <td>radiofan2</td>\n",
       "      <td>6.149054e-03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>967</td>\n",
       "      <td>1103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kaser100</th>\n",
       "      <td>306</td>\n",
       "      <td>42</td>\n",
       "      <td>4862</td>\n",
       "      <td>7319</td>\n",
       "      <td>kaser100</td>\n",
       "      <td>1.551704e-02</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>2082</td>\n",
       "      <td>1904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DeplorableRick</th>\n",
       "      <td>18</td>\n",
       "      <td>19</td>\n",
       "      <td>760</td>\n",
       "      <td>1241</td>\n",
       "      <td>DeplorableRick</td>\n",
       "      <td>5.291231e-03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>800</td>\n",
       "      <td>978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kingdomMan</th>\n",
       "      <td>89</td>\n",
       "      <td>4</td>\n",
       "      <td>1172</td>\n",
       "      <td>1969</td>\n",
       "      <td>kingdomMan</td>\n",
       "      <td>2.564034e-03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>536</td>\n",
       "      <td>1043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Whicket</th>\n",
       "      <td>490</td>\n",
       "      <td>108</td>\n",
       "      <td>16161</td>\n",
       "      <td>21862</td>\n",
       "      <td>Whicket</td>\n",
       "      <td>1.562968e-02</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>2059</td>\n",
       "      <td>1767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WeAreAllTommyRobinson</th>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>45</td>\n",
       "      <td>WeAreAllTommyRobinson</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Luther2</th>\n",
       "      <td>7</td>\n",
       "      <td>23</td>\n",
       "      <td>8</td>\n",
       "      <td>62</td>\n",
       "      <td>Luther2</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anhero23</th>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>74</td>\n",
       "      <td>157</td>\n",
       "      <td>anhero23</td>\n",
       "      <td>1.630122e-03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>185</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PewDieNazi</th>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>PewDieNazi</td>\n",
       "      <td>3.201372e-05</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MOTOFIEND</th>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "      <td>64</td>\n",
       "      <td>MOTOFIEND</td>\n",
       "      <td>1.264060e-09</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9245 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      fearspeech hatespeech normal  total  \\\n",
       "radiofan2                     47         26   1716   2287   \n",
       "kaser100                     306         42   4862   7319   \n",
       "DeplorableRick                18         19    760   1241   \n",
       "kingdomMan                    89          4   1172   1969   \n",
       "Whicket                      490        108  16161  21862   \n",
       "...                          ...        ...    ...    ...   \n",
       "WeAreAllTommyRobinson          8          2      9     45   \n",
       "Luther2                        7         23      8     62   \n",
       "anhero23                       3         10     74    157   \n",
       "PewDieNazi                     0         15     12     31   \n",
       "MOTOFIEND                      2          9     15     64   \n",
       "\n",
       "                                        name  eigen_vector  betweeness  \\\n",
       "radiofan2                          radiofan2  6.149054e-03    0.000000   \n",
       "kaser100                            kaser100  1.551704e-02    0.000055   \n",
       "DeplorableRick                DeplorableRick  5.291231e-03    0.000000   \n",
       "kingdomMan                        kingdomMan  2.564034e-03    0.000000   \n",
       "Whicket                              Whicket  1.562968e-02    0.000041   \n",
       "...                                      ...           ...         ...   \n",
       "WeAreAllTommyRobinson  WeAreAllTommyRobinson  0.000000e+00    0.000000   \n",
       "Luther2                              Luther2  0.000000e+00    0.000000   \n",
       "anhero23                            anhero23  1.630122e-03    0.000000   \n",
       "PewDieNazi                        PewDieNazi  3.201372e-05    0.000014   \n",
       "MOTOFIEND                          MOTOFIEND  1.264060e-09    0.000000   \n",
       "\n",
       "                       followers  following  \n",
       "radiofan2                    967       1103  \n",
       "kaser100                    2082       1904  \n",
       "DeplorableRick               800        978  \n",
       "kingdomMan                   536       1043  \n",
       "Whicket                     2059       1767  \n",
       "...                          ...        ...  \n",
       "WeAreAllTommyRobinson          0         21  \n",
       "Luther2                        0          5  \n",
       "anhero23                     185        256  \n",
       "PewDieNazi                     6          5  \n",
       "MOTOFIEND                      1          6  \n",
       "\n",
       "[9245 rows x 9 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'number_of_posts': 64, 'followers_count': 6, 'followings_count': 1}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_final_users['fearspeech']=df_final_users['fearspeech']/df_final_users['total']\n",
    "# df_final_users['hatespeech']=df_final_users['hatespeech']/df_final_users['total']\n",
    "dict_users_original[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_90' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-f438ab4fe042>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_90\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df_90' is not defined"
     ]
    }
   ],
   "source": [
    "df_90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/punyajoy/.conda/envs/punyajoy_gpu/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3418, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-21-147c0d4b925b>\", line 2, in <module>\n",
      "    for index,row in df_90.iterrows():\n",
      "NameError: name 'df_90' is not defined\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/punyajoy/.conda/envs/punyajoy_gpu/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2045, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'NameError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/punyajoy/.conda/envs/punyajoy_gpu/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1170, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/punyajoy/.conda/envs/punyajoy_gpu/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 316, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/punyajoy/.conda/envs/punyajoy_gpu/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 350, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/punyajoy/.conda/envs/punyajoy_gpu/lib/python3.8/inspect.py\", line 1503, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/home/punyajoy/.conda/envs/punyajoy_gpu/lib/python3.8/inspect.py\", line 1461, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/home/punyajoy/.conda/envs/punyajoy_gpu/lib/python3.8/inspect.py\", line 708, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/home/punyajoy/.conda/envs/punyajoy_gpu/lib/python3.8/inspect.py\", line 745, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-147c0d4b925b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfollowing_follower_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf_90\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'followers'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_90' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/.conda/envs/punyajoy_gpu/lib/python3.8/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2044\u001b[0m                         \u001b[0;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2045\u001b[0;31m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2046\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NameError' object has no attribute '_render_traceback_'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/punyajoy_gpu/lib/python3.8/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2045\u001b[0m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2046\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2047\u001b[0;31m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0m\u001b[1;32m   2048\u001b[0m                                             value, tb, tb_offset=tb_offset)\n\u001b[1;32m   2049\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/punyajoy_gpu/lib/python3.8/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1434\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1435\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1436\u001b[0;31m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1437\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[1;32m   1438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/punyajoy_gpu/lib/python3.8/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1334\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1335\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1336\u001b[0;31m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1337\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1338\u001b[0m             )\n",
      "\u001b[0;32m~/.conda/envs/punyajoy_gpu/lib/python3.8/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[1;32m   1194\u001b[0m                                                                tb_offset)\n\u001b[1;32m   1195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/punyajoy_gpu/lib/python3.8/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1151\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/punyajoy_gpu/lib/python3.8/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    449\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "following_follower_ratio=[]\n",
    "for index,row in df_90.iterrows():\n",
    "    if(row['followers']>10):\n",
    "        \n",
    "        following_follower_ratio.append(row['following']/row['followers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_users.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=1\n",
    "for ratio in following_follower_ratio:\n",
    "    if(ratio>10):\n",
    "        count+=1\n",
    "\n",
    "count/len(df_final_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_to_reach_list=[]\n",
    "velocity=[]\n",
    "for index,row in tqdm_notebook(df_final_users.iterrows(),total=len(df_final_users)):\n",
    "    try:\n",
    "        time=df_users_timeline[df_users_timeline['user_key']==index]['time_to_reach'].iloc[0]\n",
    "        vel=df_users_timeline[df_users_timeline['user_key']==index]['velocity'].iloc[0]\n",
    "    except IndexError:\n",
    "        time=-1\n",
    "        vel=-1\n",
    "    time_to_reach_list.append(time)\n",
    "    velocity.append(vel)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_users['time_to_reach']=time_to_reach_list\n",
    "df_final_users['velocity']=velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_users.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Follower distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    " \n",
    "\n",
    "def get_graph_features(file):\n",
    "    map_dict={}\n",
    "    follower_dict={}\n",
    "    count=0\n",
    "    with open(file) as fp:\n",
    "        dict_follower_followship=json.load(fp)\n",
    "\n",
    "    for user_key in tqdm_notebook(dict_follower_followship.keys()):\n",
    "        try:\n",
    "            temp=map_dict[user_key]\n",
    "        except KeyError:\n",
    "            map_dict[user_key]=count\n",
    "            count+=1\n",
    "\n",
    "\n",
    "\n",
    "        for user_follower in dict_follower_followship[user_key]['follower']:\n",
    "            try:\n",
    "                temp=map_dict[user_follower]\n",
    "            except KeyError:\n",
    "                map_dict[user_follower]=count\n",
    "                count+=1\n",
    "\n",
    "            try:\n",
    "                follower_dict[map_dict[user_key]][map_dict[user_follower]]=1\n",
    "            except KeyError:\n",
    "                follower_dict[map_dict[user_key]]={}\n",
    "                follower_dict[map_dict[user_key]][map_dict[user_follower]]=1\n",
    "\n",
    "    reverse_map_dict={value:key for key,value in map_dict.items()}\n",
    "\n",
    "    G = nx.DiGraph()\n",
    "    for user_key in follower_dict.keys():\n",
    "        for follower_key in follower_dict[user_key].keys():\n",
    "            G.add_edge(user_key, follower_key)\n",
    "            \n",
    "    return G, map_dict, reverse_map_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file='../../../../GAB_data/FollowersFollowing/Follow_Following_June2018.json'\n",
    "G, map_dict, reverse_map_dict=get_graph_features(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(G.nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_followers=[]\n",
    "for index,row in tqdm_notebook(df_final_users.iterrows(),total=len(df_final_users)):\n",
    "    try:\n",
    "        succesors=list(G.successors(map_dict[index]))\n",
    "        normal=0\n",
    "        total=0\n",
    "        for suc in succesors:\n",
    "            user_suc_name=reverse_map_dict[suc]\n",
    "            try:\n",
    "                temp=all_normal_dict[user_suc_name]\n",
    "                normal+=1\n",
    "            except KeyError:\n",
    "                pass\n",
    "            total+=1\n",
    "        if(total>0):\n",
    "            normal_followers.append(normal/total)\n",
    "        else:\n",
    "            normal_followers.append(0)\n",
    "    except (nx.NetworkXError, KeyError) as e:\n",
    "        normal_followers.append(-1)\n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_users['normal_followers']=normal_followers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_dict={}\n",
    "for month_year in tqdm_notebook(interaction_data.keys()):\n",
    "    user_data=interaction_data[month_year]\n",
    "    for key in user_data.keys():\n",
    "        try:\n",
    "            interaction_dict[key[0]][key[1]]+=user_data[key]['replies']\n",
    "        except KeyError:\n",
    "            try:\n",
    "                interaction_dict[key[0]][key[1]]=user_data[key]['replies']\n",
    "            except KeyError:\n",
    "                interaction_dict[key[0]]={}\n",
    "                interaction_dict[key[0]][key[1]]=user_data[key]['replies']\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_count=0\n",
    "for month_year in tqdm_notebook(interaction_data.keys()):\n",
    "    user_data=interaction_data[month_year]\n",
    "    for key in user_data.keys():\n",
    "        if(user_data[key]['replies']>0):\n",
    "            edges_count+=1\n",
    "        if(user_data[key]['reposts']>0):\n",
    "            edges_count+=1\n",
    "        if(user_data[key]['mentions']>0):\n",
    "            edges_count+=1\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx\n",
    "G = networkx.DiGraph()\n",
    "for user_key in tqdm_notebook(interaction_dict.keys()):\n",
    "    for inter_user_key in interaction_dict[user_key].keys():\n",
    "        if(user_key==inter_user_key):\n",
    "            continue\n",
    "        if(interaction_dict[user_key][inter_user_key]>0):\n",
    "            G.add_edge(inter_user_key,user_key,weight=interaction_dict[user_key][inter_user_key])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(G.edges())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_reposters=[]\n",
    "total_normal_reposts=[]\n",
    "for index,row in tqdm_notebook(df_final_users.iterrows(),total=len(df_final_users)):\n",
    "    try:\n",
    "        succesors=list(G.successors(index))\n",
    "        normal=0\n",
    "        total_normal=0\n",
    "        for user_suc_name in succesors:\n",
    "            weight=G[index][user_suc_name][\"weight\"]\n",
    "            try:\n",
    "                temp=all_normal_dict[user_suc_name]\n",
    "                normal+=weight\n",
    "                total_normal+=1\n",
    "            except KeyError:\n",
    "                pass\n",
    "            \n",
    "        normal_reposters.append(total_normal)\n",
    "        total_normal_reposts.append(normal)\n",
    "            \n",
    "    \n",
    "    except (nx.NetworkXError, KeyError) as e:\n",
    "        total_normal_reposts.append(-1)\n",
    "        normal_reposters.append(-1)\n",
    "        pass\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_users['normal_reposters']=normal_reposters\n",
    "df_final_users['total_normal_reposts']=total_normal_reposts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_users['normal_mentions']=normal_reposters\n",
    "df_final_users['total_normal_mentions']=total_normal_reposts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_users['normal_repliers']=normal_reposters\n",
    "df_final_users['total_normal_replies']=total_normal_reposts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_users.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stats model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "str_column='normal_followers'\n",
    "linear_model_str=str_column+\" ~fearspeech+hatespeech+total\"\n",
    "\n",
    "df_final_users_temp=df_final_users[['fearspeech','hatespeech','total',str_column]]\n",
    "#normalized_df=(df_final_users_temp-df_final_users_temp.min())/(df_final_users_temp.max()-df_final_users_temp.min())\n",
    "print(np.mean(df_final_users[str_column]))\n",
    "model_lin = sm.OLS.from_formula(linear_model_str,data=df_final_users_temp)\n",
    "result_lin = model_lin.fit()\n",
    "result_lin.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_column='normal_followers'\n",
    "linear_model_str=str_column+\" ~ fearspeech+hatespeech+total\"\n",
    "df_final_users_temp=df_final_users[df_final_users[str_column]!=-1]\n",
    "print(np.mean(df_final_users_temp[str_column]),len(df_final_users_temp))\n",
    "#model_lin = sm.OLS.from_formula(linear_model_str,data=df_final_users_temp[['fearspeech','hatespeech','total',str_column]])\n",
    "model_lin = sm.GLM.from_formula(linear_model_str,data=df_final_users_temp[['fearspeech','hatespeech','total',str_column]],family=sm.families.NegativeBinomial())\n",
    "result_lin = model_lin.fit()\n",
    "result_lin.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interaction network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.var(df_final_users['followers'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_90_hate = df_final_users.loc[list(df_matched.treatment)]\n",
    "df_90_hate['label']='H'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_90_hate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_90_fear = df_final_users.loc[list(df_matched.control)]\n",
    "df_90_fear['label']='F'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_90_fear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_90=pd.concat([df_90_fear,df_90_hate],ignore_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_90[df_90['label']=='F']['total'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_90_hate.time_to_reach.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "matplotlib.rcParams['figure.figsize'] = (3, 3)\n",
    "sns.set(context='paper',style='whitegrid',font_scale=1.5)\n",
    "\n",
    "# sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# tips = sns.load_dataset(\"tips\")\n",
    "ax = sns.barplot(x=\"label\",y=\"betweeness\", data=df_90, palette=['red','orange'])\n",
    "#plt.show()\n",
    "path='../Results/betweenness_centrality.pdf'\n",
    "plt.savefig(path,bbox_inches='tight',dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "matplotlib.rcParams['figure.figsize'] = (3, 3)\n",
    "sns.set(context='paper',style='whitegrid',font_scale=1.5)\n",
    "\n",
    "# sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# tips = sns.load_dataset(\"tips\")\n",
    "ax = sns.barplot(x=\"label\",y=\"eigen_vector\", data=df_90, palette=['red','orange'])\n",
    "#plt.show()\n",
    "path='../Results/eigen_vector_centrality.pdf'\n",
    "plt.savefig(path,bbox_inches='tight',dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "matplotlib.rcParams['figure.figsize'] = (3, 3)\n",
    "sns.set(context='paper',style='whitegrid',font_scale=1.5)\n",
    "\n",
    "# sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# tips = sns.load_dataset(\"tips\")\n",
    "ax = sns.barplot(x=\"label\",y=\"followers\", data=df_90, palette=['red','orange'])\n",
    "#plt.show()\n",
    "path='../Results/followers.pdf'\n",
    "plt.savefig(path,bbox_inches='tight',dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "matplotlib.rcParams['figure.figsize'] = (3, 3)\n",
    "sns.set(context='paper',style='whitegrid',font_scale=1.5)\n",
    "\n",
    "# sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# tips = sns.load_dataset(\"tips\")\n",
    "ax = sns.barplot(x=\"label\",y=\"following\", data=df_90, palette=['red','orange'])\n",
    "#plt.show()\n",
    "path='../Results/followings.pdf'\n",
    "plt.savefig(path,bbox_inches='tight',dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_90.to_csv('../Results/top_10_percentile.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "matplotlib.rcParams['figure.figsize'] = (3, 3)\n",
    "sns.set(context='paper',style='whitegrid',font_scale=1.5)\n",
    "\n",
    "# sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# tips = sns.load_dataset(\"tips\")\n",
    "ax = sns.barplot(x=\"label\",y=\"normal_followers\", data=df_90, palette=['red','orange'])\n",
    "#plt.show()\n",
    "path='../Results/normal_followers.pdf'\n",
    "plt.savefig(path,bbox_inches='tight',dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_90['normal_followers']=df_90['normal_followers']/(df_90['followers']+1e-15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import urllib, json\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/plotly/plotly.js/master/test/image/mocks/sankey_energy.json'\n",
    "response = urllib.request.urlopen(url)\n",
    "data = json.loads(response.read())\n",
    "colours=data['data'][0]['node']['color']\n",
    "\n",
    "def get_sources_target_val(df):\n",
    "    dict_tuples={}\n",
    "    for i in range(0,len(months_list)-1):\n",
    "        key=tuple([months_list[i],months_list[i+1]])\n",
    "        dict_tuples[key]={}\n",
    "        wide_df = df[[months_list[i],months_list[i+1],'freq']].groupby([months_list[i],months_list[i+1]], as_index=False).sum()\n",
    "        columns=list(wide_df.columns)[0:3]\n",
    "        for index,row in wide_df.iterrows():\n",
    "            dict_tuples[key][tuple([row[columns[0]],row[columns[1]]])]=row['freq']\n",
    "    \n",
    "    sources=[]\n",
    "    target=[]\n",
    "    values=[]\n",
    "    colour_values=[]\n",
    "    i=0\n",
    "    for key in dict_tuples.keys():\n",
    "        for element in sorted(list(dict_tuples[key].keys())):\n",
    "            print(element)\n",
    "            source_index=label[key[0]+'-'+str(element[0])]\n",
    "            target_index=label[key[1]+'-'+str(element[1])]\n",
    "#             source_index=key[0]+'-'+str(element[0])\n",
    "#             target_index=key[1]+'-'+str(element[1])\n",
    "\n",
    "            value=dict_tuples[key][element]\n",
    "            \n",
    "            \n",
    "            sources.append(source_index)\n",
    "            target.append(target_index)\n",
    "            values.append(value)\n",
    "            \n",
    "            if((element[0] in [0,1,2,99]) and (element[1] in [0,1,2,99])):\n",
    "                if(key[0]=='8/17'):\n",
    "                    colour_values.append(colours[i].replace(\"0.8\", str(0.8)))\n",
    "                else:\n",
    "                    colour_values.append(colours[i].replace(\"0.8\", str(0.4)))\n",
    "            else:\n",
    "                colour_values.append(\"rgba(0, 0, 0, 0.05)\")\n",
    "        i+=1\n",
    "    return sources,target,values,colour_values,dict_tuples\n",
    "\n",
    "\n",
    "\n",
    "def get_plotly_figure(sources,target,values,colour_values,labels_modified):\n",
    "    layout = go.Layout(\n",
    "      margin=go.layout.Margin(\n",
    "            l=1, #left margin\n",
    "            r=1, #right margin\n",
    "            b=10, #bottom margin\n",
    "            t=2  #top margin\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "    fig = go.Figure(data=[go.Sankey(\n",
    "        arrangement = \"snap\",\n",
    "        node = dict(\n",
    "          pad = 15,\n",
    "          thickness = 2,\n",
    "          line = dict(color = \"black\", width = 0.2),\n",
    "          label = labels_modified,\n",
    "          color='black'\n",
    "        ),\n",
    "        link = dict(\n",
    "          source = sources, # indices correspond to labels, eg A1, A2, A1, B1, ...\n",
    "          target = target,\n",
    "          value = values,\n",
    "          color = colours_value\n",
    "      ))],layout=layout)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook\n",
    "df_users_timeline=pd.read_csv('../Results/kcore_analysis.csv')\n",
    "df_users_timeline=df_users_timeline.drop(columns=['time_to_reach','velocity'])\n",
    "#df_users_timeline=df_users_timeline.replace([3,4,5,6,7,8,9],98)\n",
    "# df_users_timeline['freq']=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users_timeline_fear=df_users_timeline[df_users_timeline['user_key'].isin(list(df_90_fear.index))]\n",
    "df_users_timeline_hate=df_users_timeline[df_users_timeline['user_key'].isin(list(df_90_hate.index))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_tuples={}\n",
    "i=8\n",
    "key=tuple([months_list[i],months_list[i+1]])\n",
    "dict_tuples[key]={}\n",
    "wide_df = df_users_timeline_fear[[months_list[i],months_list[i+1],'freq']].groupby([months_list[i],months_list[i+1]], as_index=False).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months_list=list(df_users_timeline.columns)[2:-1]\n",
    "\n",
    "label={}\n",
    "count=0\n",
    "for month in months_list:\n",
    "    for i in range(0,10,1):\n",
    "        label[month+'-'+str(i)]=count\n",
    "        count+=1        \n",
    "    label[month+'-'+str(99)]=count\n",
    "    count+=1\n",
    "\n",
    "labels_modified=[]\n",
    "for ele in list(label.keys()):\n",
    "    if ele.split('-')[1]=='99':\n",
    "        labels_modified.append('NA')\n",
    "    else:\n",
    "        labels_modified.append(ele.split('-')[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources,target,values,colours_value,dict_tuples=get_sources_target_val(df_users_timeline_fear)\n",
    "fig = get_plotly_figure(sources,target,values,colours_value,labels_modified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.write_image(\"../Results/sankey_fear.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources,target,values,colours_value,dict_tuples=get_sources_target_val(df_users_timeline_hate)\n",
    "fig = get_plotly_figure(sources,target,values,colours_value,labels_modified)\n",
    "fig.write_image(\"../Results/sankey_hate.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "matplotlib.rcParams['figure.figsize'] = (3, 3)\n",
    "sns.set(context='paper',style='whitegrid',font_scale=1.5)\n",
    "\n",
    "# sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# tips = sns.load_dataset(\"tips\")\n",
    "ax = sns.barplot(x=\"label\",y=\"time_to_reach\", data=df_90, palette=['red','orange'])\n",
    "#plt.show()\n",
    "path='../Results/time_to_reach.pdf'\n",
    "plt.savefig(path,bbox_inches='tight',dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_90.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "matplotlib.rcParams['figure.figsize'] = (3, 3)\n",
    "sns.set(context='paper',style='whitegrid',font_scale=1.5)\n",
    "\n",
    "# sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# tips = sns.load_dataset(\"tips\")\n",
    "ax = sns.barplot(x=\"label\",y=\"normal_reposters\", data=df_90, palette=['red','orange'])\n",
    "#plt.show()\n",
    "path='../Results/normal_reposters.pdf'\n",
    "plt.savefig(path,bbox_inches='tight',dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "matplotlib.rcParams['figure.figsize'] = (3, 3)\n",
    "sns.set(context='paper',style='whitegrid',font_scale=1.5)\n",
    "\n",
    "# sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# tips = sns.load_dataset(\"tips\")\n",
    "ax = sns.barplot(x=\"label\",y=\"total_normal_reposts\", data=df_90, palette=['red','orange'])\n",
    "#plt.show()\n",
    "path='../Results/total_normal_reposts.pdf'\n",
    "plt.savefig(path,bbox_inches='tight',dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "matplotlib.rcParams['figure.figsize'] = (3, 3)\n",
    "sns.set(context='paper',style='whitegrid',font_scale=1.5)\n",
    "\n",
    "# sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# tips = sns.load_dataset(\"tips\")\n",
    "ax = sns.barplot(x=\"label\",y=\"normal_mentions\", data=df_90, palette=['red','orange'])\n",
    "#plt.show()\n",
    "path='../Results/normal_mentions.pdf'\n",
    "plt.savefig(path,bbox_inches='tight',dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "matplotlib.rcParams['figure.figsize'] = (3, 3)\n",
    "sns.set(context='paper',style='whitegrid',font_scale=1.5)\n",
    "\n",
    "# sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# tips = sns.load_dataset(\"tips\")\n",
    "ax = sns.barplot(x=\"label\",y=\"total_normal_mentions\", data=df_90, palette=['red','orange'])\n",
    "#plt.show()\n",
    "path='../Results/total_normal_mentions.pdf'\n",
    "plt.savefig(path,bbox_inches='tight',dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "matplotlib.rcParams['figure.figsize'] = (3, 3)\n",
    "sns.set(context='paper',style='whitegrid',font_scale=1.5)\n",
    "\n",
    "# sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# tips = sns.load_dataset(\"tips\")\n",
    "ax = sns.barplot(x=\"label\",y=\"normal_repliers\", data=df_90, palette=['red','orange'])\n",
    "#plt.show()\n",
    "path='../Results/normal_repliers.pdf'\n",
    "plt.savefig(path,bbox_inches='tight',dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "matplotlib.rcParams['figure.figsize'] = (3, 3)\n",
    "sns.set(context='paper',style='whitegrid',font_scale=1.5)\n",
    "\n",
    "# sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# tips = sns.load_dataset(\"tips\")\n",
    "ax = sns.barplot(x=\"label\",y=\"total_normal_replies\", data=df_90, palette=['red','orange'])\n",
    "#plt.show()\n",
    "path='../Results/total_normal_repliers.pdf'\n",
    "plt.savefig(path,bbox_inches='tight',dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_90.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "from numpy.random import randn\n",
    "from scipy.stats import mannwhitneyu\n",
    "seed(1)\n",
    "column_name='normal_followers'\n",
    "\n",
    "treatment=list(df_90[(df_90['label']=='F') & (df_90[column_name]!=-1)][column_name])\n",
    "control=list(df_90[(df_90['label']=='H') & (df_90[column_name]!=-1)][column_name])\n",
    "print(\"F\",np.mean(treatment))\n",
    "print(\"H\",np.mean(control))\n",
    "\n",
    "stat, p = mannwhitneyu(treatment, control,alternative='greater')\n",
    "print('Statistics=%.3f, p=%.10f' % (stat, p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top10user=list(df_90[df_90['label']=='F'].sort_values('total_normal_reposts',ascending=False)[0:10].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def get_samples(users=None,label='fearspeech'):\n",
    "    threshold={'hatespeech':0.9,'fearspeech':0.7,'normal':0.7}\n",
    "    posts=[]\n",
    "    files = sorted(glob('../../Gab_Data/new_features_old_gab/gab_fear_hate_features*.pickle'))\n",
    "    for file in tqdm_notebook(files,total=len(files)):\n",
    "   \n",
    "        with open(file, 'rb') as handle:\n",
    "            Gab_keyword_match = pickle.load(handle)\n",
    "\n",
    "        if('predicted_probab' not in Gab_keyword_match[1].keys()):\n",
    "            print(\"not found\")\n",
    "            continue\n",
    "\n",
    "        for element in Gab_keyword_match:\n",
    "            \n",
    "            post={}\n",
    "            post['body']=element['post_body']\n",
    "            labels=[]\n",
    "    \n",
    "            for i in range(len(element['predicted_probab'])):\n",
    "                if(element['predicted_probab'][i]>threshold[reverse_dict_labels[i]]):\n",
    "                    labels.append(reverse_dict_labels[i])\n",
    "\n",
    "            post['labels']=labels\n",
    "            \n",
    "            if(label=='normal' and post['labels']==[]):\n",
    "                if(element['username'] in users):\n",
    "                    #print(line,labels)\n",
    "                    posts.append([element['post_body'],'normal',element['like_count'],element['repost_count']])\n",
    "            \n",
    "            if(post['labels']==[label]):\n",
    "                if(element['username'] in users):\n",
    "#                     #print(line,labels)\n",
    "                    posts.append([element['post_body'],labels[0],element['like_count'],element['repost_count'],element['reply_count']])\n",
    "    print(len(posts))\n",
    "    df=pd.DataFrame(posts, columns=['text','label','like','repost','reply'])\n",
    "    return df\n",
    "#     print(len(posts))\n",
    "#     samples=random.sample(posts,min(len(posts),20))\n",
    "    \n",
    "#     for sample in samples:\n",
    "#         print(sample)\n",
    "#         print(\"==============================================\")\n",
    "\n",
    "def get_samples_date(month_start=1,month_end=3,year_start=2017,year_end=2017,label='fearspeech'):\n",
    "    threshold={'hatespeech':0.9,'fearspeech':0.7,'normal':0.7}\n",
    "    posts=[]\n",
    "    files = sorted(glob('../../Gab_Data/new_features_old_gab/gab_fear_hate_features*.pickle'))\n",
    "    for file in tqdm_notebook(files,total=len(files)):\n",
    "   \n",
    "        with open(file, 'rb') as handle:\n",
    "            Gab_keyword_match = pickle.load(handle)\n",
    "\n",
    "        if('predicted_probab' not in Gab_keyword_match[1].keys()):\n",
    "            print(\"not found\")\n",
    "            continue\n",
    "\n",
    "        for element in Gab_keyword_match:\n",
    "            flag_add=False\n",
    "            try:\n",
    "                dt = datetime.fromisoformat(element['post_create_time'])\n",
    "                if((dt.month >= month_start and dt.month <= month_end) and (dt.year >= year_start and dt.year <= year_end)):\n",
    "                    flag_add=True\n",
    "                else:\n",
    "                    flag_add=False\n",
    "#                 key_new=str(dt.month)+'/'+str(dt.year)\n",
    "            except KeyError:\n",
    "                continue\n",
    "            \n",
    "            \n",
    "            post={}\n",
    "            post['body']=element['post_body']\n",
    "            labels=[]\n",
    "            \n",
    "            \n",
    "            \n",
    "            for i in range(len(element['predicted_probab'])):\n",
    "                if(element['predicted_probab'][i]>threshold[reverse_dict_labels[i]]):\n",
    "                    labels.append(reverse_dict_labels[i])\n",
    "\n",
    "            post['labels']=labels\n",
    "            \n",
    "            if(label=='normal' and post['labels']==[]):\n",
    "#                 if(element['username'] in users):\n",
    "#                     #print(line,labels)\n",
    "                    if(flag_add):\n",
    "                        posts.append([element['post_body'],'normal',element['like_count'],element['repost_count']])\n",
    "            \n",
    "            if(post['labels']==[label]):\n",
    "#                 if(element['username'] in users):\n",
    "#                     #print(line,labels)\n",
    "                    if(flag_add):\n",
    "                        posts.append([element['post_body'],labels[0],element['like_count'],element['repost_count'],element['reply_count']])\n",
    "    print(len(posts))\n",
    "    df=pd.DataFrame(posts, columns=['text','label','like','repost','reply'])\n",
    "    return df\n",
    "#     print(len(posts))\n",
    "#     samples=random.sample(posts,min(len(posts),20))\n",
    "    \n",
    "#     for sample in samples:\n",
    "#         print(sample)\n",
    "#         print(\"==============================================\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=get_samples(users=None,label='fearspeech')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values('reply',ascending=False)[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mentions=get_samples_date(month_start=2,month_end=3,year_start=2018,year_end=2018,label='hatespeech')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mentions_hate=get_samples_date(month_start=2,month_end=3,year_start=2018,year_end=2018,label='hatespeech')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mentions_hate.sort_values('repost',ascending=False)[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mentioned_dict_highest={}\n",
    "for index,row in df_mentions.iterrows():\n",
    "    text=row['text'].split()\n",
    "    for word in text:\n",
    "        mentioned_user=None\n",
    "        if(len(word.split('@'))>1):\n",
    "            res = word.split('@')[1]\n",
    "\n",
    "            # TO AVOID GMAILS\n",
    "            if(len(res.split('.'))>1):\n",
    "                if_gmail=res.split('.')[1]\n",
    "                if(if_gmail!='com'):\n",
    "                    mentioned_user=res\n",
    "            else:\n",
    "                mentioned_user=res\n",
    "        \n",
    "        try:\n",
    "            mentioned_dict_highest[mentioned_user]+=1\n",
    "        except KeyError:\n",
    "            mentioned_dict_highest[mentioned_user]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "    for ele in list(df_mentions.sample(10).text):\n",
    "        print(ele)\n",
    "        print(\"------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hate=get_samples(users=['JaneDoeCountry', 'st3pp3nw0lf', 'Sterangeli5', 'WarRoomShow', 'Lynne75', 'Castelnau', 'Kaybird', 'Pamisue', 'LesaJoy', 'Didizichi'],label='normal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hate.to_csv('mentioned_nromal.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "df.sort_values(by='like',ascending=False).head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('../../../../Gab_Data_old/user_details.json') as fp:\n",
    "    dict_users_original = json.load(fp)\n",
    "\n",
    "exfearuser=list(df_90[df_90['label']=='F'].sort_values('total_normal_reposts',ascending=False).index)\n",
    "def get_mentions(text):\n",
    "    mentioned_users=[]\n",
    "    text=text.split()\n",
    "    for word in text:\n",
    "        muser=None\n",
    "        if(len(word.split('@'))>1):\n",
    "            res = word.split('@')[1]\n",
    "            # TO AVOID GMAILS\n",
    "            if(len(res.split('.'))>1):\n",
    "                if_gmail=res.split('.')[1]\n",
    "                if(if_gmail!='com'):\n",
    "                    muser=res\n",
    "            else:\n",
    "                muser=res\n",
    "\n",
    "                    \n",
    "            if(muser!=None):\n",
    "                try:\n",
    "                    temp=dict_users_original[muser]\n",
    "                    mentioned_users.append(muser)\n",
    "                except KeyError:\n",
    "                    continue\n",
    "    return mentioned_users\n",
    "        \n",
    "        \n",
    "def get_mentioned_users(users=exfearuser,label='fearspeech'):\n",
    "    threshold={'hatespeech':0.9,'fearspeech':0.7,'normal':0.7}\n",
    "    posts=[]\n",
    "    mentioned_user_dict={}\n",
    "    files = sorted(glob('../../Gab_Data/new_features_old_gab/gab_fear_hate_features*.pickle'))\n",
    "    for file in tqdm_notebook(files,total=len(files)):\n",
    "   \n",
    "        with open(file, 'rb') as handle:\n",
    "            Gab_keyword_match = pickle.load(handle)\n",
    "\n",
    "        if('predicted_probab' not in Gab_keyword_match[1].keys()):\n",
    "            print(\"not found\")\n",
    "            continue\n",
    "\n",
    "        for element in Gab_keyword_match:\n",
    "            \n",
    "            post={}\n",
    "            post['body']=element['post_body']\n",
    "            \n",
    "            \n",
    "            labels=[]\n",
    "    \n",
    "            for i in range(len(element['predicted_probab'])):\n",
    "                if(element['predicted_probab'][i]>threshold[reverse_dict_labels[i]]):\n",
    "                    labels.append(reverse_dict_labels[i])\n",
    "\n",
    "            post['labels']=labels\n",
    "            \n",
    "            if(label=='normal' and post['labels']==[]):\n",
    "                if(element['username'] in users):\n",
    "                    #print(line,labels)\n",
    "                    posts.append([element['post_body'],'normal',element['like_count'],element['repost_count']])\n",
    "            \n",
    "            if(post['labels']==[label]):\n",
    "                if(element['username'] in users):\n",
    "                    #print(line,labels)\n",
    "                    \n",
    "                    mentioned_users=get_mentions(element['post_body'])\n",
    "                    \n",
    "                    \n",
    "                    for element in mentioned_users:\n",
    "                        try:\n",
    "                            temp=all_normal_dict[element]\n",
    "                            try:\n",
    "                                mentioned_user_dict[element]+=1\n",
    "                            except KeyError:\n",
    "                                mentioned_user_dict[element]=1\n",
    "                        except:\n",
    "                            pass\n",
    "                    \n",
    "                                \n",
    "                    \n",
    "                    #posts.append([element['post_body'],labels[0],element['like_count'],element['repost_count']])\n",
    "    return mentioned_user_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hybrid Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mentioned_user_dict=get_mentioned_users(users=exfearuser,label='fearspeech')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_dict={k: v for k, v in sorted(mentioned_user_dict.items(), key=lambda item: item[1], reverse=True)[0:10]}\n",
    "sorted_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_follower=0\n",
    "avg_following=0 \n",
    "avg_post=0\n",
    "for key in sorted_dict.keys():\n",
    "    if(key=='WarRoomShow'):\n",
    "        continue\n",
    "    \n",
    "    avg_follower+=dict_users_original[key]['follower_count']/10\n",
    "    avg_following+=dict_users_original[key]['following_count']/10\n",
    "    avg_post+=dict_users_original[key]['post_count']/10\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(avg_follower,avg_following,avg_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_dict={}\n",
    "exfearuser=list(df_90[df_90['label']=='F'].sort_values('total_normal_reposts',ascending=False).index)\n",
    "exhateuser=list(df_90[df_90['label']=='H'].sort_values('total_normal_reposts',ascending=False).index)\n",
    "\n",
    "\n",
    "def check_condition(users,key,user_data,type_interaction='replies'):\n",
    "    if(type_interaction=='replies'):\n",
    "        if(key[1] in users) and (key[0] in all_normal_dict) and user_data[key][type_interaction]>0:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    elif(type_interaction=='reposts'):\n",
    "        if(key[0] in users) and (key[1] in all_normal_dict) and user_data[key][type_interaction]>0:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    elif(type_interaction=='mentions'):\n",
    "        if(key[0] in users) and (key[1] in all_normal_dict) and user_data[key][type_interaction]>0:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "\n",
    "        \n",
    "def get_interaction_dict(interaction_data,users,type_interaction):\n",
    "    interaction_dict={}\n",
    "    for month_year in tqdm_notebook(interaction_data.keys()):\n",
    "        user_data=interaction_data[month_year]\n",
    "        user_replied={}\n",
    "        for key in user_data.keys():\n",
    "\n",
    "            if(check_condition(users,key,user_data,type_interaction=type_interaction)):\n",
    "\n",
    "                try:\n",
    "\n",
    "                    interaction_dict[month_year]['total_interaction']+=user_data[key][type_interaction]\n",
    "                    interaction_dict[month_year]['total_users']+=1\n",
    "\n",
    "                    try:\n",
    "                        temp=user_replied[key[0]]\n",
    "                    except KeyError:\n",
    "                        user_replied[key[0]]=1\n",
    "                        interaction_dict[month_year]['unique_users']+=1\n",
    "\n",
    "                except KeyError:\n",
    "                    interaction_dict[month_year]={}\n",
    "                    interaction_dict[month_year]['total_interaction']=user_data[key][type_interaction]\n",
    "                    interaction_dict[month_year]['total_users']=1\n",
    "\n",
    "\n",
    "                    try:\n",
    "                        temp=user_replied[key[0]]\n",
    "                    except KeyError:\n",
    "                        user_replied[key[0]]=1\n",
    "                        interaction_dict[month_year]['unique_users']=1\n",
    "    return interaction_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interaction_dict_exfear_reply=get_interaction_dict(interaction_data,exfearuser,'replies')\n",
    "# interaction_dict_exhate_reply=get_interaction_dict(interaction_data,exhateuser,'replies')\n",
    "# interaction_dict_exfear_repost=get_interaction_dict(interaction_data,exfearuser,'reposts')\n",
    "# interaction_dict_exhate_repost=get_interaction_dict(interaction_data,exhateuser,'reposts')\n",
    "interaction_dict_exfear_mention=get_interaction_dict(interaction_data,exfearuser,'mentions')\n",
    "interaction_dict_exhate_mention=get_interaction_dict(interaction_data,exhateuser,'mentions')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.gridspec as gridspec\n",
    "matplotlib.rcParams['figure.figsize'] = (20, 5)\n",
    "sns.set(context='paper',style='whitegrid',font_scale=2, rc={\"lines.linewidth\": 2.0,\"lines.markersize\": 10})\n",
    "\n",
    "x=[]\n",
    "y=[]\n",
    "\n",
    "for key in interaction_dict_exfear_mention:\n",
    "    x.append(key[0:2]+'-'+key[-2:])\n",
    "    y.append(interaction_dict_exfear_mention[key]['total_interaction']/interaction_dict_exfear_mention[key]['unique_users'])\n",
    "\n",
    "sns.lineplot(x=x, y=y, color='red')\n",
    "\n",
    "x=[]\n",
    "y=[]\n",
    "\n",
    "for key in interaction_dict_exhate_mention:\n",
    "    x.append(key[0:2]+'-'+key[-2:])\n",
    "    y.append(interaction_dict_exhate_mention[key]['total_interaction']/interaction_dict_exhate_mention[key]['unique_users'])\n",
    "\n",
    "sns.lineplot(x=x, y=y,color='orange')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_dict_exhate_mention[key]['unique_users']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get fractions replied to by normal users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = sorted(glob('../../Gab_Data/new_features_old_gab/gab_fear_hate_features*.pickle'))\n",
    "\n",
    "#dict_posts_ids={}\n",
    "for file in tqdm_notebook(files,total=len(files)):\n",
    "    with open(file, 'rb') as handle:\n",
    "        Gab_keyword_match = pickle.load(handle)\n",
    "        \n",
    "    if('predicted_probab' not in Gab_keyword_match[1].keys()):\n",
    "        print(\"not found\")\n",
    "        continue\n",
    "        \n",
    "    for element in Gab_keyword_match:\n",
    "        post={}\n",
    "        post['body']=element['post_body']\n",
    "\n",
    "\n",
    "        labels=[]\n",
    "\n",
    "        for i in range(len(element['predicted_probab'])):\n",
    "            if(element['predicted_probab'][i]>threshold[reverse_dict_labels[i]]):\n",
    "                labels.append(reverse_dict_labels[i])\n",
    "\n",
    "        post['labels']=labels\n",
    "\n",
    "        if(label=='normal' and post['labels']==[]):\n",
    "            pass\n",
    "#         elif(post['labels']==['fearspeech'] or post['labels']==['hatespeech']):\n",
    "#             dict_posts_ids[element['id']]['label']=post['labels'][0]\n",
    "        \n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dict_post_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-punyajoy_gpu] *",
   "language": "python",
   "name": "conda-env-.conda-punyajoy_gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
