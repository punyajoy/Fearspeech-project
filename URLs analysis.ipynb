{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle5 as pickle\n",
    "from glob import glob\n",
    "from datetime import datetime\n",
    "import re\n",
    "import numpy as np\n",
    "from matplotlib.colors import ListedColormap\n",
    "from scipy.stats import mannwhitneyu\n",
    "threshold={'hatespeech':0.9,'fearspeech':0.7,'normal':0.5}\n",
    "dict_labels={'hatespeech':2,'fearspeech':1,'normal':0}\n",
    "reverse_dict_labels={dict_labels[key]:key for key in dict_labels.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import urllib.parse as urlparse\n",
    "import emoji\n",
    "\n",
    "\n",
    "GENERIC_TLDS = [\n",
    "    'aero', 'asia', 'biz', 'com', 'coop', 'edu', 'gov', 'info', 'int', 'jobs', \n",
    "    'mil', 'mobi', 'museum', 'name', 'net', 'org', 'pro', 'tel', 'travel', 'cat'\n",
    "    ]\n",
    "\n",
    "def get_domain(url):\n",
    "    hostname = urlparse.urlparse(url.lower()).netloc\n",
    "    if hostname == '':\n",
    "        # Force the recognition as a full URL\n",
    "        hostname = urlparse.urlparse('http://' + url).netloc\n",
    "\n",
    "    # Remove the 'user:passw', 'www.' and ':port' parts\n",
    "    hostname = hostname.split('@')[-1].split(':')[0].lstrip('www.').split('.')\n",
    "\n",
    "    num_parts = len(hostname)\n",
    "    if (num_parts < 3) or (len(hostname[-1]) > 2):\n",
    "        return '.'.join(hostname[:-1])\n",
    "    if len(hostname[-2]) > 2 and hostname[-2] not in GENERIC_TLDS:\n",
    "        return '.'.join(hostname[:-1])\n",
    "    if num_parts >= 3:\n",
    "        return '.'.join(hostname[:-2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_urls={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-386-9a0e75efb92c>:2: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for file in tqdm_notebook(files,total=len(files)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f9f1b9590cc4e17b43e8f3e3624a33c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/212 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "files = sorted(glob('../../Gab_Data/new_features_old_gab/gab_fear_hate_features*.pickle'))\n",
    "for file in tqdm_notebook(files,total=len(files)):\n",
    "    with open(file, 'rb') as handle:\n",
    "        Gab_keyword_match = pickle.load(handle)\n",
    "        \n",
    "    if('predicted_probab' not in Gab_keyword_match[1].keys()):\n",
    "        continue\n",
    "    \n",
    "    for element in Gab_keyword_match:\n",
    "        try:\n",
    "            dt = datetime.fromisoformat(element['post_create_time'])\n",
    "            key_new=str(dt.month)+'/'+str(dt.year)\n",
    "        except KeyError:\n",
    "            continue\n",
    "            \n",
    "        \n",
    "        \n",
    "        temp=element['post_body']\n",
    "        all_urls = re.findall('https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+', temp)\n",
    "        \n",
    "        labels=[]\n",
    "\n",
    "        for i in range(len(element['predicted_probab'])):\n",
    "            if(element['predicted_probab'][i]>threshold[reverse_dict_labels[i]]):\n",
    "                labels.append(reverse_dict_labels[i])\n",
    "        \n",
    "        for url in all_urls:\n",
    "            if('fearspeech' in labels):\n",
    "                try:\n",
    "                    dict_urls[get_domain(url)][url]['fearspeech']+=1\n",
    "                except KeyError:\n",
    "                    try:\n",
    "                        dict_urls[get_domain(url)][url]={}\n",
    "                        dict_urls[get_domain(url)][url]['fearspeech']=1\n",
    "                    except KeyError:\n",
    "                        dict_urls[get_domain(url)]={}\n",
    "                        dict_urls[get_domain(url)][url]={}\n",
    "                        dict_urls[get_domain(url)][url]['fearspeech']=1\n",
    "\n",
    "                            \n",
    "                        \n",
    "            elif('hatespeech' in labels):\n",
    "                try:\n",
    "                    dict_urls[get_domain(url)][url]['hatespeech']+=1\n",
    "                except KeyError:\n",
    "                    try:\n",
    "                        dict_urls[get_domain(url)][url]={}\n",
    "                        dict_urls[get_domain(url)][url]['hatespeech']=1\n",
    "                    except KeyError:\n",
    "                        dict_urls[get_domain(url)]={}\n",
    "                        dict_urls[get_domain(url)][url]={}\n",
    "                        dict_urls[get_domain(url)][url]['hatespeech']=1\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "            else:\n",
    "                pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_urls(url,label):\n",
    "    count=0\n",
    "    files = sorted(glob('../../Gab_Data/new_features_old_gab/gab_fear_hate_features*.pickle'))\n",
    "    for file in tqdm_notebook(files,total=len(files)):\n",
    "            with open(file, 'rb') as handle:\n",
    "                Gab_keyword_match = pickle.load(handle)\n",
    "\n",
    "            if('predicted_probab' not in Gab_keyword_match[1].keys()):\n",
    "                continue\n",
    "\n",
    "            for element in Gab_keyword_match:\n",
    "                try:\n",
    "                    dt = datetime.fromisoformat(element['post_create_time'])\n",
    "                    key_new=str(dt.month)+'/'+str(dt.year)\n",
    "                except KeyError:\n",
    "                    continue\n",
    "\n",
    "\n",
    "\n",
    "                temp=element['post_body']\n",
    "                all_urls = re.findall('https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+', temp)\n",
    "\n",
    "                labels=[]\n",
    "\n",
    "                for i in range(len(element['predicted_probab'])):\n",
    "                    if(element['predicted_probab'][i]>threshold[reverse_dict_labels[i]]):\n",
    "                        labels.append(reverse_dict_labels[i])\n",
    "\n",
    "                    \n",
    "                if(label in labels):\n",
    "                    if(url in all_urls):\n",
    "                        if(count>10):\n",
    "                            break\n",
    "                        print(element['post_body'])\n",
    "                        count+=1\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_fear={}\n",
    "dict_hate={}\n",
    "dict_normal={}\n",
    "\n",
    "for key in dict_urls.keys():\n",
    "    for url_key in dict_urls[key].keys():\n",
    "        try:\n",
    "            dict_fear[key]+=dict_urls[key][url_key]['fearspeech']\n",
    "        except KeyError:\n",
    "            try:\n",
    "                dict_fear[key]=dict_urls[key][url_key]['fearspeech']\n",
    "            except KeyError:\n",
    "                pass\n",
    "        \n",
    "        try:\n",
    "            dict_hate[key]+=dict_urls[key][url_key]['hatespeech']\n",
    "        except KeyError:\n",
    "            try:\n",
    "                dict_hate[key]=dict_urls[key][url_key]['hatespeech']\n",
    "            except KeyError:\n",
    "                pass\n",
    "        \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    " with open('../Results/url_information.json', 'r') as openfile:\n",
    "        # Reading from json file\n",
    "        url_ground_truth = json.load(openfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_ground_truth={}\n",
    "for key in url_ground_truth:\n",
    "    domain_ground_truth[get_domain(key)]=url_ground_truth[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Factual Reporting': ' MIXED',\n",
       " 'Country': ' USA',\n",
       " 'World Press Freedom Rank': ' USA 45/180'}"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domain_ground_truth['easelzippers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weaselzippers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_fear_urls={k: v for k, v in sorted(dict_fear.items(), key=lambda item: item[1],reverse=True)[0:20]}\n",
    "top_hate_urls={k: v for k, v in sorted(dict_hate.items(), key=lambda item: item[1],reverse=True)[0:20]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aclj {'http://www.aclj.org': {'fearspeech': 7}, 'http://aclj.us': {'fearspeech': 215}, 'http://aclj.org': {'fearspeech': 1}, 'https://aclj.us': {'fearspeech': 19}, 'https://aclj.org': {'fearspeech': 1}} 243\n",
      "================\n",
      "kek {'https://kek.gg': {'hatespeech': 1}, 'https://www.kek.gg': {'hatespeech': 1}, 'http://kek.gg': {'fearspeech': 159}} 159\n",
      "================\n",
      "islamexposedblog.blogspot {'http://islamexposedblog.blogspot.com': {'fearspeech': 70}, 'https://islamexposedblog.blogspot.co.uk': {'fearspeech': 2}} 72\n",
      "================\n",
      "dlvr {'http://dlvr.it': {'fearspeech': 67}, 'https://dlvr.it': {'fearspeech': 1}} 68\n",
      "================\n",
      "freecharitycars {'http://www.freecharitycars.org': {'fearspeech': 66}, 'http://freecharitycars.org': {'fearspeech': 2}} 68\n",
      "================\n",
      "nationaleconomicseditorial {'http://www.nationaleconomicseditorial.com': {'fearspeech': 47}, 'https://www.nationaleconomicseditorial.com': {'fearspeech': 7}, 'https://nationaleconomicseditorial.com': {'fearspeech': 4}} 58\n",
      "================\n",
      "hitenationnetwork {'http://www.whitenationnetwork.com': {'fearspeech': 54}} 54\n",
      "================\n",
      "1.cbn {'http://www1.cbn.com': {'fearspeech': 42}, 'https://www1.cbn.com': {'fearspeech': 4}} 46\n",
      "================\n",
      "ift {'https://ift.tt': {'fearspeech': 8}, 'http://ift.tt': {'fearspeech': 36}} 44\n",
      "================\n",
      "theamerican-messenger {'http://www.theamerican-messenger.com': {'fearspeech': 42}} 42\n",
      "================\n",
      "thereligionofpeace {'https://www.thereligionofpeace.com': {'fearspeech': 22}, 'http://www.thereligionofpeace.com': {'fearspeech': 1}, 'http://thereligionofpeace.com': {'fearspeech': 9}, 'https://thereligionofpeace.com': {'fearspeech': 7}, 'http://www.TheReligionOfPeace.com': {'fearspeech': 1}} 40\n",
      "================\n",
      "imgoat {'https://imgoat.com': {'fearspeech': 1}, 'http://imgoat.com': {'fearspeech': 37}, 'http://www.imgoat.com': {'hatespeech': 1}, 'https://www.imgoat.com': {'fearspeech': 1}} 39\n",
      "================\n",
      "telchaination.blogspot {'http://telchaination.blogspot.com': {'fearspeech': 39}} 39\n",
      "================\n",
      "israelvideonetwork {'http://www.israelvideonetwork.com': {'fearspeech': 34}, 'https://www.israelvideonetwork.com': {'fearspeech': 5}} 39\n",
      "================\n",
      "sptnkne {'https://sptnkne.ws': {'fearspeech': 37}, 'http://sptnkne.ws': {'hatespeech': 1}} 37\n",
      "================\n",
      "patriotmusicnetwork {'https://www.patriotmusicnetwork.com': {'fearspeech': 36}} 36\n",
      "================\n",
      "newspage {'http://newspage.co.za': {'fearspeech': 35}} 35\n",
      "================\n",
      "counterjihad {'http://counterjihad.com': {'fearspeech': 28}, 'https://counterjihad.com': {'fearspeech': 5}} 33\n",
      "================\n",
      " {'http://www.zero': {'fearspeech': 1}, 'http://www.post-': {'fearspeech': 1}, 'https://p': {'fearspeech': 1}, 'https://pi': {'fearspeech': 4}, 'http://www.cisorg': {'fearspeech': 1}, 'https://t': {'hatespeech': 1}, 'https://tw': {'hatespeech': 1}, 'https://twi': {'fearspeech': 2}, 'http://bit': {'fearspeech': 1}, 'https://freedomoutpost': {'fearspeech': 3}, 'http://thegatewaypundi': {'fearspeech': 1}, 'https://twitt': {'fearspeech': 1}, 'http://liveleak': {'fearspeech': 1}, 'https://twitter': {'fearspeech': 1}, 'http://www.illegalaliencrim': {'fearspeech': 1}, 'http://SaveThePersecute': {'fearspeech': 1}, 'http://greateststorynevertold': {'fearspeech': 1}, 'http://shttps': {'fearspeech': 1}, 'http://www.westmonst': {'fearspeech': 1}, 'http://www.nytimes': {'fearspeech': 1}, 'https://www.gatestoneiThe': {'fearspeech': 1}, 'http://vaccines': {'fearspeech': 1}, 'https://twit': {'fearspeech': 1}, 'https://conservativetribu': {'fearspeech': 1}, 'https://CommunismByThebackdoor': {'fearspeech': 1}, 'http://mutilation%20scenes%20in%20lars%20von%20trier%27s%20new%20film%20cause%20100%20to%20walk%20out%20in%20cannes': {'fearspeech': 1}, 'http://freedom': {'fearspeech': 1}, 'http://whoolisblog': {'hatespeech': 1}, 'http://www.centerforsecuritypo': {'fearspeech': 1}, 'https://rfair3': {'fearspeech': 1}} 33\n",
      "================\n",
      "tapyoureit.boards {'http://tapyoureit.boards.net': {'fearspeech': 31}} 31\n",
      "================\n"
     ]
    }
   ],
   "source": [
    "top_fear_df=[]\n",
    "\n",
    "for key in top_fear_urls.keys():\n",
    "    print(key,dict_urls[key],top_fear_urls[key])\n",
    "    print(\"================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pagesix {'https://pagesix.com': {'hatespeech': 65}, 'http://pagesix.com': {'hatespeech': 29}} 94\n",
      "================\n",
      "towleroad {'http://www.towleroad.com': {'hatespeech': 68}} 68\n",
      "================\n",
      "rodentguys {'https://www.rodentguys.com': {'hatespeech': 60}} 60\n",
      "================\n",
      "thewrap {'https://www.thewrap.com': {'hatespeech': 45}, 'http://www.thewrap.com': {'hatespeech': 11}} 56\n",
      "================\n",
      "easelzippers {'http://www.weaselzippers.us': {'hatespeech': 4}, 'https://www.weaselzippers.us': {'hatespeech': 42}} 46\n",
      "================\n",
      "pp.userapi {'https://pp.userapi.com': {'hatespeech': 36}} 36\n",
      "================\n",
      "bbs.dailystormer {'http://bbs.dailystormer.com': {'fearspeech': 1}, 'https://bbs.dailystormer.com': {'hatespeech': 32}, 'https://bbs.dailystormer.lol': {'hatespeech': 1}} 33\n",
      "================\n",
      "pornwikileaks {'http://www.pornwikileaks.com': {'hatespeech': 32}} 32\n",
      "================\n",
      "dailystormer {'http://www.dailystormer.com': {'fearspeech': 7}, 'https://www.dailystormer.com': {'fearspeech': 1}, 'https://dailystormer.ph': {'fearspeech': 1}, 'https://dailystormer.ai': {'hatespeech': 4}, 'https://dailystormer.red': {'hatespeech': 2}, 'https://dailystormer.name': {'hatespeech': 1}, 'https://dailystormer.top': {'fearspeech': 1}, 'https://dailystormer.ws': {'hatespeech': 1}, 'https://dailystormer.hk': {'hatespeech': 3}, 'http://dailystormer.name': {'hatespeech': 1}, 'https://dailystormer.at': {'hatespeech': 4}, 'https://dailystormer.is': {'hatespeech': 1}, 'https://dailystormer.cat': {'hatespeech': 1}, 'https://dailystormer.lol': {'fearspeech': 1}, 'https://dailystormer.al': {'hatespeech': 8}, 'http://www.dailystormer.hk': {'hatespeech': 1}, 'http://dailystormer.ca': {'fearspeech': 1}, 'https://dailystormer.ru': {'fearspeech': 1}, 'http://dailystormer.al': {'hatespeech': 1}, 'http://dailystormer.lol': {'hatespeech': 1}, 'http://dailystormer.at': {'hatespeech': 1}} 30\n",
      "================\n",
      "godhatesfags {'http://GodHatesFags.com': {'hatespeech': 25}, 'http://www.godhatesfags.com': {'hatespeech': 1}, 'http://GodHATESfags.com': {'hatespeech': 1}, 'http://godhatesfags.com': {'hatespeech': 1}} 28\n",
      "================\n",
      "78.media.tumblr {'https://78.media.tumblr.com': {'hatespeech': 24}, 'http://78.media.tumblr.com': {'hatespeech': 3}} 27\n",
      "================\n",
      "hollywoodreporter {'http://www.hollywoodreporter.com': {'hatespeech': 6}, 'https://www.hollywoodreporter.com': {'hatespeech': 19}} 25\n",
      "================\n",
      "twitchy {'http://twitchy.com': {'hatespeech': 21}, 'https://twitchy.com': {'hatespeech': 3}} 24\n",
      "================\n",
      "voxday.blogspot {'http://voxday.blogspot.com': {'fearspeech': 1}, 'https://voxday.blogspot.com': {'hatespeech': 6}, 'https://voxday.blogspot.co.at': {'fearspeech': 1}, 'https://voxday.blogspot.com.au': {'hatespeech': 1}, 'https://voxday.blogspot.de': {'hatespeech': 1}, 'http://voxday.blogspot.kr': {'hatespeech': 5}, 'http://voxday.blogspot.co.uk': {'hatespeech': 1}, 'http://voxday.blogspot.ciom': {'fearspeech': 1}, 'https://voxday.blogspot.hu': {'hatespeech': 1}, 'http://voxday.blogspot.nl': {'fearspeech': 1}, 'http://voxday.blogspot.ch': {'fearspeech': 1}, 'https://voxday.blogspot.jp': {'hatespeech': 1}, 'http://voxday.blogspot.tw': {'hatespeech': 1}, 'http://voxday.blogspot.ca': {'hatespeech': 1}, 'http://voxday.blogspot.ie': {'fearspeech': 1}, 'https://voxday.blogspot.ch': {'fearspeech': 1}, 'https://voxday.blogspot.ca': {'fearspeech': 1}, 'http://voxday.blogspot.sg': {'fearspeech': 1}, 'http://voxday.blogspot.in': {'hatespeech': 1}, 'https://voxday.blogspot.co.uk': {'hatespeech': 1}, 'https://voxday.blogspot.nl': {'fearspeech': 1}, 'http://voxday.blogspot.co.nz': {'hatespeech': 1}} 21\n",
      "================\n",
      "supremecourt {'https://www.supremecourt.gov': {'hatespeech': 21}} 21\n",
      "================\n",
      "prntscr {'http://prntscr.com': {'hatespeech': 19}, 'https://prntscr.com': {'hatespeech': 1}} 20\n",
      "================\n",
      "urbandictionary {'https://www.urbandictionary.com': {'hatespeech': 20}, 'http://www.urbandictionary.com': {'fearspeech': 1}} 20\n",
      "================\n",
      "thesmokinggun {'http://thesmokinggun.com': {'hatespeech': 2}, 'http://www.thesmokinggun.com': {'hatespeech': 18}} 20\n",
      "================\n",
      "imageshack {'http://imageshack.com': {'hatespeech': 8}, 'https://imageshack.com': {'hatespeech': 11}} 19\n",
      "================\n",
      "mediaite {'http://www.mediaite.com': {'fearspeech': 1}, 'https://www.mediaite.com': {'hatespeech': 19}} 19\n",
      "================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for key in top_hate_urls.keys():\n",
    "    print(key,dict_urls[key],top_hate_urls[key])\n",
    "    print(\"================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-405-c4f3e90d1d1e>:4: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for file in tqdm_notebook(files,total=len(files)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "063dd2d9d6e041caa0a77314f1a88e9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/212 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THIS DIRT BAG NEEDS TO BE PUBLICLY ____  ____  _____! \n",
      "(you should know those 3 words I cant say legally)\n",
      "https://pagesix.com/2017/10/10/harvey-weinstein-accused-of-sexual-assault-in-another-expose/\n",
      "BYE BYE Anti- Trump  scumbag https://pagesix.com/2017/10/10/harvey-weinstein-accused-of-sexual-assault-in-another-expose/\n",
      "Because it's a Negro thing.\n",
      "\n",
      "https://pagesix.com/2018/01/14/dennis-rodman-arrested-for-dui/\n",
      "Negro rapper flaps his fat mouth once too often, once too much.\n",
      "\n",
      "https://pagesix.com/2018/01/25/flavor-flav-beaten-up-in-las-vegas-casino/\n",
      "ffs...\n",
      "\n",
      "the nigger is a goddamn rapist.  I know, I know, he was just doing what a Nigger does according to his nature and he got away with it for years - lived long enough to enjoy his fame and his riches...SmDh\n",
      "\n",
      "They say Old Nigger went berserk in the courtroom after  hearing his conviction.\n",
      "\n",
      "https://pagesix.com/2018/04/27/cosbys-rep-compares-trial-to-emmett-till-lynching/\n",
      "Leave it to insane liberals, and  soon we'll get a transvestite transgender pygmy Muslim in a headbag 'James' Bond.\n",
      "\n",
      "https://pagesix.com/2018/02/12/rachel-weisz-james-bond-shouldnt-be-played-by-a-woman/\n",
      "Alec Baldwin another #asswipe routes his #leftyanger towards #myPresidentTrump 'If you don't like our president then move to another country!!!'\n",
      "https://pagesix.com/2016/12/23/alec-baldwin-wants-to-perform-highway-to-hell-at-trump-inauguration/\n",
      "George Lopez booed off stage after Trump jokes flop at gala\n",
      "\n",
      "Stupid beaner\n",
      "\n",
      "https://pagesix.com/2017/10/14/george-lopez-booed-off-stage-after-trump-jokes-flop-at-gala/?_ga=2.50705199.769395375.1507991516-1934498675.1507991516\n",
      "https://pagesix.com/2017/10/14/george-lopez-booed-off-stage-after-trump-jokes-flop-at-gala/\n",
      "The Latin Alec Baldwin, POS.\n",
      "Hahahahahahahahaha let's crucify the \"savior\" then. Fat shit Weinstein can eat a bullet. https://pagesix.com/2017/10/30/weinstein-tells-pals-scandal-happened-so-he-could-change-the-world/amp/\n",
      "And this fat dyke by her words, spends 90 percent of her time obsessing over Trump!\n",
      "\n",
      "So how much time is left for her dyke companion?\n",
      "\n",
      "How much FOOD is left?\n",
      "\n",
      "https://pagesix.com/2017/11/07/rosie-odonnells-new-girlfriend-is-22-years-her-junior/\n"
     ]
    }
   ],
   "source": [
    "get_sample_urls('https://pagesix.com','hatespeech')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'http://www.nationaleconomicseditorial.com': {'fearspeech': 47},\n",
       " 'https://www.nationaleconomicseditorial.com': {'fearspeech': 7},\n",
       " 'https://nationaleconomicseditorial.com': {'fearspeech': 4}}"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_urls['nationaleconomicseditorial']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Archived data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'all-links'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-90b9766a6573>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0murls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"all-links\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"final-result\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"a\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfinal_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlink\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'all-links'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "\n",
    "def parse_page(link):\n",
    "    resp = requests.get(link)\n",
    "\n",
    "    if resp.status_code == 404:\n",
    "        return\n",
    "    soup = BeautifulSoup(resp.content, 'html.parser')\n",
    "\n",
    "    # get name\n",
    "    site_name = soup.find_all(\"h1\", class_=\"page-title\")[0].text.strip(' \\t\\n\\r')\n",
    "\n",
    "    # get bias icon\n",
    "    if site_name == \"NDTV\":\n",
    "        bias_icon = \"leftcenter06\"\n",
    "    elif site_name == \"Knoxville News Sentinel\":\n",
    "        bias_icon = \"leftcenter06\"\n",
    "    elif site_name == \"Free Wheel Media\":\n",
    "        bias_icon = \"leftcenter06\"\n",
    "    elif site_name == \"Egyptian Streets\":\n",
    "        bias_icon = \"leftcenter06\"\n",
    "    elif site_name == \"The Times and Democrat\":\n",
    "        bias_icon = \"leastbiased011\"\n",
    "    else:\n",
    "        img = soup.find_all(\"div\", class_=\"container mh-mobile\")[0].find_all(\"img\")[:1]\n",
    "        src = img[0][\"src\"]\n",
    "        pattern = re.compile(\"http.*\\/([a-zA-z0-9]*)\\.png\")\n",
    "        ms = pattern.match(src)\n",
    "        bias_icon = ms.group(1)\n",
    "\n",
    "    # get factual reporting rating\n",
    "    factual_reporting_rating = None\n",
    "    pgs = soup.find_all(\"p\") \n",
    "    for pg in pgs:\n",
    "        if (\"Factual News:\" in pg.text or \"Factual Reporting:\" in pg.text):\n",
    "            try: \n",
    "                factual_reporting_rating = pg.find_all(\"strong\")[0].text.strip('\\n\\r')\n",
    "            except:\n",
    "                try:\n",
    "                    factual_reporting_rating = pg.find_all(\"b\")[0].text.strip('\\n\\r')\n",
    "                except:\n",
    "                    factual_reporting_rating = pg.find_all(\"span\")[0].text.strip('\\n\\r')\n",
    "\n",
    "\n",
    "    if not factual_reporting_rating:\n",
    "        divs = soup.find_all(\"div\") \n",
    "        for div in divs:\n",
    "            if (\"Factual News:\" in div.text or \"Factual Reporting:\" in div.text):\n",
    "                try: \n",
    "                    factual_reporting_rating = div.find_all(\"strong\")[0].text.strip('\\n\\r')\n",
    "                except:\n",
    "                    try:\n",
    "                        factual_reporting_rating = div.find_all(\"b\")[0].text.strip('\\n\\r')\n",
    "                    except:\n",
    "                        factual_reporting_rating = div.find_all(\"span\")[0].text.strip('\\n\\r')\n",
    "\n",
    "    if not factual_reporting_rating:\n",
    "        raise Exception(\"no mas\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Get Domain\n",
    "    if \"Elko Daily Free Press\" == site_name:\n",
    "        domain=\"https://elkodaily.com/\"\n",
    "    elif \"American Enterprise Institute\" == site_name:\n",
    "        domain=\"https://www.aei.org/\"\n",
    "    elif \"Act.TV\" == site_name:\n",
    "        domain=\"http://act.tv\"\n",
    "    elif \"The Fucking News\" == site_name:\n",
    "        domain=\"http://thefingnews.com/\"\n",
    "    elif \"Kyiv Post\" == site_name:\n",
    "        domain=\"https://www.kyivpost.com/\"\n",
    "    elif \"Philadelphia Tribune\" == site_name:\n",
    "        domain=\"http://www.phillytrib.com/\"\n",
    "    elif \"Reuters\" == site_name:\n",
    "        domain=\"http://www.reuters.com/\"\n",
    "    elif \"South Bend Tribune\" == site_name:\n",
    "        domain=\"https://www.southbendtribune.com/\"\n",
    "    elif \"ConservativeOpinion.com\" == site_name:\n",
    "        domain=\"https://conservativeopinion.com/\"\n",
    "    else:    \n",
    "        content = soup.find_all(\"div\", class_=\"main\")[0]\n",
    "        anchors = content.find_all(\"a\")\n",
    "        for anchor in anchors:\n",
    "            if (anchor.get(\"href\", False) and \n",
    "               (site_name == \"Wikipedia\" or \"wikipedia\" not in anchor[\"href\"]) and\n",
    "               (\"Sources:\" in anchor.parent.text or \"Notes:\" in anchor.parent.text or \"Source:\" in anchor.parent.text)):\n",
    "\n",
    "                domain = anchor[\"href\"]\n",
    "\n",
    "    return site_name, domain, bias_icon, factual_reporting_rating\n",
    "\n",
    "urls = []\n",
    "with open(\"all-links\", \"r\") as f:\n",
    "    with open(\"final-result\", \"a\") as final_result:\n",
    "        for link in f:\n",
    "            res = parse_page(link.strip('\\t\\n\\r'))\n",
    "            if res:\n",
    "                print(res)\n",
    "                final_result.write(\",\".join(res) + \"\\n\")\n",
    "\n",
    "\n",
    "#link = 'https://mediabiasfactcheck.com/reuters/'\n",
    "#print(parse_page(link))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls=[\n",
    "    'https://mediabiasfactcheck.com/pseudoscience-dictionary/',\n",
    "    'https://mediabiasfactcheck.com/left/',\n",
    "    'https://mediabiasfactcheck.com/center/',\n",
    "    'https://mediabiasfactcheck.com/leftcenter/',\n",
    "    'https://mediabiasfactcheck.com/right/',\n",
    "    'https://mediabiasfactcheck.com/right-center/',\n",
    "    'https://mediabiasfactcheck.com/conspiracy/',\n",
    "    'https://mediabiasfactcheck.com/fake-news/',\n",
    "    'https://mediabiasfactcheck.com/pro-science/',\n",
    "    'https://mediabiasfactcheck.com/satire/'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-134-277904cc1df4>:6: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for url in tqdm_notebook(urls):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1597a817d9e948d5805e94322cd789c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm,tqdm_notebook\n",
    "all_links=[]\n",
    "\n",
    "for url in tqdm_notebook(urls):\n",
    "    text = requests.get(url)\n",
    "    soup = BeautifulSoup(text.content,'html.parser')\n",
    "    table=soup.find('div',{'id':'mh-wrapper'}).find('div',{'class':'mh-section mh-group'}).find('article').find('table')\n",
    "    links = table.findAll('a')\n",
    "    all_links+=links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a href=\"https://mediabiasfactcheck.com/pseudoscience-dictionary-5g-coronavirus-conspiracy/\"><span style=\"font-size: 12pt;\">5G-Coronavirus Conspiracy</span></a>"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###\n",
    "dict_urls={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_search_list=['Bias Rating',\n",
    " 'Factual Reporting',\n",
    " 'Country',\n",
    " 'Press Freedom Rating',\n",
    " 'Media Type',\n",
    " 'Traffic/Popularity',\n",
    " 'MBFC Credibility Rating']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-211-2c545c83f228>:4: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for link in tqdm_notebook(all_links[591:]):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dca0322d25b1430c85dcf655e7f2fddb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# dict_urls={}\n",
    "\n",
    "\n",
    "for link in tqdm_notebook(all_links[591:]):\n",
    "    try:\n",
    "        resp = requests.get(link['href'])\n",
    "    except gaierror:\n",
    "        continue\n",
    "    soup = BeautifulSoup(resp.content, 'html.parser')\n",
    "    try:\n",
    "        paragraphs=soup.findAll('article')[0].findAll('p')\n",
    "    except IndexError:\n",
    "        continue\n",
    "    dict_temp={}\n",
    "    ## search for bias rating\n",
    "    flag=0\n",
    "    for para in paragraphs:\n",
    "        for ele in to_search_list:\n",
    "            if(ele in para.text):\n",
    "                try:\n",
    "                    lists=para.text.split('\\n')\n",
    "                    dict_temp={element.split(\":\")[0]:element.split(\":\")[1].replace(u'\\xa0', u'') for element in lists}\n",
    "                    flag+=1\n",
    "                    break\n",
    "                except IndexError:\n",
    "                    pass\n",
    "            if('Source' in para.text):\n",
    "                try:\n",
    "                    url=para.text.split('Source:')[1].replace(u'\\xa0', u'')\n",
    "                    flag+=1\n",
    "                    break\n",
    "                except IndexError:\n",
    "                    pass\n",
    "\n",
    "                \n",
    "            \n",
    "        if(flag==2):\n",
    "            dict_urls[url]=dict_temp\n",
    "            break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' https://www.accountable.us/'"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(dict_urls.keys())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Results/url_information.json', 'w') as fp:\n",
    "    json.dump(dict_urls, fp,indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Source: https://speld.nl/']"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.findAll('article')[0].findAll('p')[8].text.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'https://mediabiasfactcheck.com/pseudoscience-dictionary-5g-coronavirus-conspiracy/': {},\n",
       " 'https://mediabiasfactcheck.com/pseudoscience-dictionary-acupressure/': {},\n",
       " 'https://mediabiasfactcheck.com/pseudoscience-directory-acupuncture/': {},\n",
       " 'https://mediabiasfactcheck.com/pseudoscience-dictionary-alternative-cancer-treatments/': {},\n",
       " 'https://mediabiasfactcheck.com/pseudoscience-dictionary-alternative-medicine/': {},\n",
       " 'https://mediabiasfactcheck.com/pseudoscience-dictionary-aids-denialism/': {},\n",
       " 'https://mediabiasfactcheck.com/pseudoscience-dictionary-ancient-astronauts/': {},\n",
       " 'https://mediabiasfactcheck.com/pseudoscience-dictionary-anti-vaccination-anti-vax/': {},\n",
       " 'https://mediabiasfactcheck.com/pseudoscience-dictionary-applied-kinesiology/': {},\n",
       " 'https://mediabiasfactcheck.com/pseudoscience-dictionary-aroma-therapy/': {}}"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bias Rating',\n",
       " 'Factual Reporting',\n",
       " 'Country',\n",
       " 'Press Freedom Rating',\n",
       " 'Media Type',\n",
       " 'Traffic/Popularity',\n",
       " 'MBFC Credibility Rating']"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_search_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-punyajoy_gpu] *",
   "language": "python",
   "name": "conda-env-.conda-punyajoy_gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
