{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "import json\n",
    "from glob import glob\n",
    "from datetime import datetime\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "#import time\n",
    "from time import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "file_list = list(Path('../../../../GAB_data/FollowersFollowing').glob(\"*.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('../../../../GAB_data/FollowersFollowing/Follow_Following_March2018.json'),\n",
       " PosixPath('../../../../GAB_data/FollowersFollowing/Follow_Following_May2017.json'),\n",
       " PosixPath('../../../../GAB_data/FollowersFollowing/Follow_Following_August2017.json'),\n",
       " PosixPath('../../../../GAB_data/FollowersFollowing/Follow_Following_March2017.json'),\n",
       " PosixPath('../../../../GAB_data/FollowersFollowing/Follow_Following_April2017.json'),\n",
       " PosixPath('../../../../GAB_data/FollowersFollowing/Follow_Following_February2017.json'),\n",
       " PosixPath('../../../../GAB_data/FollowersFollowing/Follow_Following_October2017.json'),\n",
       " PosixPath('../../../../GAB_data/FollowersFollowing/Follow_Following_November2016.json'),\n",
       " PosixPath('../../../../GAB_data/FollowersFollowing/Follow_Following_September2017.json'),\n",
       " PosixPath('../../../../GAB_data/FollowersFollowing/Follow_Following_April2018.json'),\n",
       " PosixPath('../../../../GAB_data/FollowersFollowing/Follow_Following_May2018.json'),\n",
       " PosixPath('../../../../GAB_data/FollowersFollowing/Follow_Following_January2018.json'),\n",
       " PosixPath('../../../../GAB_data/FollowersFollowing/Follow_Following_December2017.json'),\n",
       " PosixPath('../../../../GAB_data/FollowersFollowing/Follow_Following_November2017.json'),\n",
       " PosixPath('../../../../GAB_data/FollowersFollowing/Follow_Following_December2016.json'),\n",
       " PosixPath('../../../../GAB_data/FollowersFollowing/Follow_Following_June2017.json'),\n",
       " PosixPath('../../../../GAB_data/FollowersFollowing/Follow_Following_October2016.json'),\n",
       " PosixPath('../../../../GAB_data/FollowersFollowing/Follow_Following_February2018.json'),\n",
       " PosixPath('../../../../GAB_data/FollowersFollowing/Follow_Following_January2017.json'),\n",
       " PosixPath('../../../../GAB_data/FollowersFollowing/Follow_Following_June2018.json'),\n",
       " PosixPath('../../../../GAB_data/FollowersFollowing/Follow_Following_July2017.json')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "user='Meepo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_list[0]) as fp:\n",
    "    dict1=json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_list[1]) as fp:\n",
    "    dict2=json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_list[7]) as fp:\n",
    "    dict4=json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17, 9, 3)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dict1[user]['follower']),len(dict2[user]['follower']),len(dict4[user]['follower'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DeadNotSleeping', 'creative', 'Hazelnut']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict4[user]['follower']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'follower': ['nosenvoisons1980',\n",
       "  'grunovconra1988',\n",
       "  'pansparleoniph1980',\n",
       "  'populistbrief',\n",
       "  'Hunteroid',\n",
       "  'perciasima1971',\n",
       "  'karlcuxx',\n",
       "  'MICHAELANTONIO',\n",
       "  'hazysun',\n",
       "  'ThomasDuder',\n",
       "  'TexasYankee4',\n",
       "  'FlatRealm',\n",
       "  'Bill_Clinton',\n",
       "  'KetzerHexe',\n",
       "  'DeadNotSleeping',\n",
       "  'creative',\n",
       "  'Hazelnut'],\n",
       " 'following': ['ThomasDuder',\n",
       "  'lift',\n",
       "  'Hazelnut',\n",
       "  'creative',\n",
       "  'DeadNotSleeping',\n",
       "  'OpenQuotes',\n",
       "  'RightSmarts']}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict1[user]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hazysun',\n",
       " 'ThomasDuder',\n",
       " 'TexasYankee4',\n",
       " 'FlatRealm',\n",
       " 'Bill_Clinton',\n",
       " 'KetzerHexe',\n",
       " 'DeadNotSleeping',\n",
       " 'creative',\n",
       " 'Hazelnut']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " dict2[user]['follower'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Posts[3090001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('../Networks/interaction_network_per_month.json', 'rb') as fp:\n",
    "    req=pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../../../Gab_Data_old/Final_Posts.pkl','rb') as f:\n",
    "    Posts = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in Posts.keys():\n",
    "    if(Posts[key]['is_repost']==True):\n",
    "        print(\"yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('../../Gab_Data/dict_user_postRepost.pklgz','rb') as fp:\n",
    "    dict_repost = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "length=0\n",
    "for key in dict_repost.keys():\n",
    "    length+=len(dict_repost[key]['reposts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-a3b2a68e2050>:2: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for key in tqdm_notebook(dict_repost.keys()):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2699ddad86d4abca6a338d5a1346677",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/184264 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-16-191012f050e9>:7: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for file in tqdm_notebook(files,total=len(files)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74a5e84aaeef478a84b6cc7a0a742829",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/212 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n",
      "loading dataset\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# GETTING THE DICTIONARY OF POSTID AS KEYS AND THE USERNAME OF THAT POST ID AS VALUE\n",
    "\n",
    "postid_username_dict={}\n",
    "\n",
    "files = sorted(glob('../../Gab_Data/new_features_old_gab/gab_fear_hate_features*.pickle'))\n",
    "\n",
    "for file in tqdm_notebook(files,total=len(files)):\n",
    "#     print(\"loading dataset\")\n",
    "    with open(file, 'rb') as handle:\n",
    "        Gab_keyword_match = pickle.load(handle)\n",
    "        \n",
    "    if('predicted_probab' not in Gab_keyword_match[1].keys()):\n",
    "        print(\"not found\")\n",
    "        continue\n",
    "        \n",
    "    for element in Gab_keyword_match:\n",
    "        postid_username_dict[element['id']]=element['username']\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('../Networks/postid_username_dict.json', 'wb') as fp:\n",
    "    pickle.dump(postid_username_dict, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('../../../../Gab_Data_old/user_details.json') as fp:\n",
    "    dict_users_original = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-17-76289e5719e5>:22: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for file in tqdm_notebook(files,total=len(files)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9b22776fffa45729b8e10f101550519",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/212 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Required is : 18.0188 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "total_dict={}\n",
    "count=0\n",
    "k=0\n",
    "no_parent_username_found=[]\n",
    "\n",
    "# Creating Keys for Total dictionary to aviod key errors latter and also to prevent sorting dictionary on basis of months later \n",
    "# 2016 year to 2019 year for all 12 months\n",
    "\n",
    "total_dict={}\n",
    "years=[2016,2017,2018,2019]\n",
    "months=[1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "for year in years:\n",
    "    for month in months:\n",
    "        date_time=str(month)+'-'+str(year)\n",
    "        total_dict[date_time]={}\n",
    "        \n",
    "\n",
    "\n",
    "# TRAVERSING THROUGH FILES AND DATA and UPDATING THE TOTAL_DICT\n",
    "files = sorted(glob('../../Gab_Data/new_features_old_gab/gab_fear_hate_features*.pickle'))\n",
    "for file in tqdm_notebook(files,total=len(files)):\n",
    "    \n",
    "    #k=k+1\n",
    "    #if (k>2):\n",
    "    #    break\n",
    "        \n",
    "    #print(\"loading dataset\")\n",
    "    with open(file, 'rb') as handle:\n",
    "        Gab_keyword_match = pickle.load(handle)\n",
    "        \n",
    "    if('predicted_probab' not in Gab_keyword_match[1].keys()):\n",
    "        print(\"not found\")\n",
    "        continue\n",
    "        \n",
    "    for element in Gab_keyword_match:\n",
    "        \n",
    "        count=count+1\n",
    "        try:\n",
    "            dt = datetime.fromisoformat(element['post_create_time'])\n",
    "        except KeyError:\n",
    "            continue\n",
    "   \n",
    "        post={}\n",
    "        \n",
    "        try:\n",
    "            post['month']=str(dt.month)\n",
    "            post['year']=str(dt.year)\n",
    "            post['username']=element['username']\n",
    "            post['id']=element['id']\n",
    "            post['body']=element['post_body']\n",
    "        except KeyError:\n",
    "            continue\n",
    "        \n",
    "   \n",
    "    \n",
    "        #Updating total dictionary FOR MENTIONS\n",
    "        #TUPLE (A,B) MEANS A IS MENTIONING B IN THE POST\n",
    "        \n",
    "        mentioning_time=post['month']+ '-' +post['year']\n",
    "        text=element['post_body']\n",
    "        \n",
    "        text=text.split()\n",
    "        for word in text:\n",
    "            mentioned_user=None\n",
    "            if(len(word.split('@'))>1):\n",
    "                res = word.split('@')[1]\n",
    "                \n",
    "                # TO AVOID GMAILS\n",
    "                if(len(res.split('.'))>1):\n",
    "                    if_gmail=res.split('.')[1]\n",
    "                    if(if_gmail!='com'):\n",
    "                        mentioned_user=res\n",
    "                else:\n",
    "                    mentioned_user=res\n",
    "                    \n",
    "                    \n",
    "            if(mentioned_user!=None):\n",
    "                try:\n",
    "                    temp=dict_users_original[mentioned_user]\n",
    "                except KeyError:\n",
    "                    continue\n",
    "                    \n",
    "            \n",
    "            if(mentioned_user!=None):\n",
    "                try:\n",
    "                    total_dict[mentioning_time][(element['username'],mentioned_user)]['mentions']+=1\n",
    "                except KeyError:\n",
    "                    total_dict[mentioning_time][(element['username'],mentioned_user)]= {'reposts':0,'replies':0,'mentions':0}\n",
    "                    total_dict[mentioning_time][(element['username'],mentioned_user)]['mentions']+=1\n",
    "            \n",
    "        \n",
    "        \n",
    "        #Updating total dictionary FOR REPOSTS\n",
    "        #TUPLE (A,B) Means B is reposting A's post\n",
    "        \n",
    "        if(dict_repost[post['id']]!=[]):\n",
    "     \n",
    "            for sample in dict_repost[post['id']]:\n",
    "                reposter_username=sample['reposter_username']\n",
    "                dt = datetime.fromisoformat(sample['reposting_time'])\n",
    "                reposting_time=str(dt.month)+ '-' + str(dt.year)\n",
    "                \n",
    "                try:\n",
    "                    total_dict[reposting_time][(element['username'],reposter_username)]['reposts']+=1\n",
    "                except KeyError:\n",
    "                    try:\n",
    "                        total_dict[reposting_time][(element['username'],reposter_username)]={'reposts':0,'replies':0,'mentions':0}\n",
    "                        total_dict[reposting_time][(element['username'],reposter_username)]['reposts']+=1\n",
    "                    except KeyError:\n",
    "                        total_dict[reposting_time]={}\n",
    "                        total_dict[reposting_time][(element['username'],reposter_username)]={'reposts':0,'replies':0,'mentions':0}\n",
    "                        total_dict[reposting_time][(element['username'],reposter_username)]['reposts']+=1\n",
    "\n",
    "        \n",
    "        \n",
    "        #Updating total dictionary FOR REPLIES\n",
    "        #TUPLE (A,B) MEANS A IS REPLYING TO B\n",
    "        \n",
    "        replying_time=post['month']+ '-' +post['year']\n",
    "      \n",
    "        if(element['parent_id']!=None):\n",
    "            try:\n",
    "                total_dict[replying_time][(element['username'],postid_username_dict[element['parent_id']])]['replies']+=1\n",
    "            except KeyError:\n",
    "                try:\n",
    "                    total_dict[replying_time][(element['username'],postid_username_dict[element['parent_id']])]= {'reposts':0,'replies':0,'mentions':0}\n",
    "                    total_dict[replying_time][(element['username'],postid_username_dict[element['parent_id']])]['replies']+=1 \n",
    "                except KeyError:\n",
    "                    no_parent_username_found.append(element['parent_id'])\n",
    "                    continue\n",
    "        \n",
    "        \n",
    "    #print(\"datasets loaded\")\n",
    "    \n",
    "    \n",
    "print(f\"Time Required is : {round((time() - t) / 60, 4)} mins\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'no_parent_username_found' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-720c65da7d5f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mno_parent_username_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'no_parent_username_found' is not defined"
     ]
    }
   ],
   "source": [
    "len(no_parent_username_found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "reposts=0\n",
    "for time in total_dict:\n",
    "    for users in total_dict[time]:\n",
    "        reposts=reposts+total_dict[time][users]['mentions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2347709"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reposts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_dict_modified={}\n",
    "\n",
    "for key in total_dict:\n",
    "    if(len(total_dict[key])>0):\n",
    "        total_dict_modified[key]=total_dict[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('../Networks/interaction_network_per_month.json', 'wb') as fp:\n",
    "    pickle.dump(total_dict_modified, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'total_dict_modified' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a4874627f6af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_dict_modified\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'total_dict_modified' is not defined"
     ]
    }
   ],
   "source": [
    "len(total_dict_modified.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from netgraph import Graph, InteractiveGraph, EditableGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
